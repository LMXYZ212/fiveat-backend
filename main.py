from fastapi import FastAPI, File, UploadFile, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
from typing import List, Dict, Any, Optional
import pandas as pd
import re
from sentence_transformers import SentenceTransformer, util
import torch
import requests
import subprocess
import tempfile
import os

from fastapi import UploadFile, Form
import requests
import base64


# ===================== 1) Initialize FastAPI + CORS =====================
app = FastAPI(
    title="NEVO Food Nutrition API (Extended)",
    description="""
Extended version: 
 - /api/text (original text input)
 - /api/image (Google Vision)
 - /api/audio (OpenAI Whisper)
 - /api/confirm (user-edited items)
""",
    version="2.1.0"
)
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # or specify your frontend origin
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# ===================== 2) Load SentenceTransformer Model =====================
model = SentenceTransformer('all-MiniLM-L6-v2')

# ===================== 3) Load NEVO Excel =====================
base_dir = os.path.dirname(os.path.abspath(__file__))

# æ‹¼æ¥å‡º Excel æ–‡ä»¶è·¯å¾„
NEVO_FILE = os.path.join(base_dir, "NEVO2023_database.xlsx")

# å°è¯•è¯»å–
try:
    nevo_df = pd.read_excel(NEVO_FILE)
    nevo_df.columns = [c.strip() for c in nevo_df.columns]
except FileNotFoundError as e:
    raise RuntimeError(f"æœªæ‰¾åˆ° NEVO Excel æ–‡ä»¶ï¼Œè¯·ç¡®ä¿ 'NEVO2023_database.xlsx' ä¸ main.py ä½äºåŒä¸€ç›®å½•ä¸‹ã€‚") from e

# Clean food names
nevo_df["Food name"] = nevo_df["Food name"].astype(str).str.replace(",", "").str.strip()
nevo_df["CleanName"] = nevo_df["Food name"].str.lower()

# Encode all NEVO food names
nevo_embeddings = model.encode(nevo_df["CleanName"].tolist(), convert_to_tensor=True)

# ===================== 4) Data Cleaning =====================
numeric_cols = ["Calories", "Fat", "protein", "Carbs"]
for col in numeric_cols:
    if col in nevo_df.columns:
        nevo_df[col] = pd.to_numeric(nevo_df[col], errors="coerce")

def wheel_group(cat: str) -> Optional[str]:
    if not isinstance(cat, str):
        return None
    c = cat.lower()
    if "fats and oils" in c:
        return "Fats"
    if "vegetables" in c or "fruit" in c:
        return "Veg&Fruit"
    if any(x in c for x in ["bread", "cereals", "potatoes", "rice"]):
        return "Carbs"
    if any(x in c for x in ["milk", "cheese", "egg", "meat", "fish", "nut"]):
        return "Protein"
    if "beverages" in c:
        return "Drinks"
    return None

nevo_df["WheelGroup"] = nevo_df["Category"].apply(wheel_group)

# ===================== 5) Food Extraction (existing logic) =====================
def extract_food_items_simple(text: str) -> List[Dict[str, Any]]:
    """
    Simple regex approach to parse '100 grams apple' or '2 pieces cheese' from text.
    If no match, fallback to each word => quantity=1.
    """
    pattern = r"(\d+(?:\.\d+)?)\s*(grams?|g|ml|kg|l|pieces?|oz)?\s+([a-zA-Z ]+?)(?:,|and|\.|$)"
    matches = re.findall(pattern, text)

    results = []
    for qty_str, unit, food in matches:
        food_cleaned = food.strip().lower()
        try:
            qty = float(qty_str)
        except:
            qty = 1.0
        results.append({
            "foodName": food_cleaned,
            "quantity": qty
        })

    if not results:
        tokens = text.split()
        for word in tokens:
            if word.isalpha():
                results.append({
                    "foodName": word.lower(),
                    "quantity": 1.0
                })
    return results

# ===================== 6) SentenceTransformer Matching =====================
def vector_match_food(user_food: str, threshold: float = 0.6) -> Optional[str]:
    query_emb = model.encode(user_food.strip().lower(), convert_to_tensor=True)
    cos_scores = util.cos_sim(query_emb, nevo_embeddings)[0]
    top_result = torch.argmax(cos_scores).item()
    top_score = cos_scores[top_result].item()
    if top_score < threshold:
        return None
    return nevo_df.iloc[top_result]["CleanName"]

def get_nutrition_info(row: pd.Series, qty: float) -> Dict[str, Any]:
    """
    NEVO data is per 100g => multiply by factor = qty/100
    """
    factor = qty / 100.0
    def scale(val):
        if pd.notna(val):
            return round(float(val)*factor, 2)
        return None

    return {
        "Calories": scale(row.get("Calories")),
        "Protein": scale(row.get("protein")),
        "Fat": scale(row.get("Fat")),
        "Carbs": scale(row.get("Carbs")),
        "Category": row.get("Category"),
        "WheelGroup": row.get("WheelGroup")
    }

# ===================== 7) Original /api/text Endpoint =====================
class TextInput(BaseModel):
    text: str

class ParsedFoodItem(BaseModel):
    foodName: str
    quantity: float
    unit: Optional[str] = None
    estimatedCategory: Optional[str] = None
    estimatedWeight: Optional[float] = None


@app.post("/api/text")
def parse_text(input: TextInput):
    items = extract_food_items_simple(input.text)
    output = []
    for item in items:
        fName, qty = item["foodName"], item["quantity"]
        matched = vector_match_food(fName)
        if matched is None:
            output.append({
                "foodName": fName,
                "quantity": qty,
                "NEVO_matched": None,
                "error": "No semantic match found in NEVO"
            })
            continue

        matched_row = nevo_df[nevo_df["CleanName"] == matched]
        if matched_row.empty:
            output.append({
                "foodName": fName,
                "quantity": qty,
                "NEVO_matched": matched,
                "error": "Matched name not found in NEVO database"
            })
            continue

        row = matched_row.iloc[0]
        info = get_nutrition_info(row, qty)
        output.append({
            "foodName": fName,
            "quantity": qty,
            "NEVO_matched": row["Food name"],
            **info
        })
    return {"foodItems": output}

@app.post("/api/text-parse")
def text_parse(input: TextInput):
    items = extract_food_items_simple(input.text)
    output = []
    seen = set()

    for it in items:
        name = it["foodName"]
        qty = it["quantity"]
        unit = it.get("unit")

        matched = vector_match_food(name)
        category, est_weight = None, None

        if matched:
            matched_row = nevo_df[nevo_df["CleanName"] == matched]
            if not matched_row.empty:
                row = matched_row.iloc[0]
                category = row.get("WheelGroup")
                est_weight = 100.0

        final_name = matched if matched else name

        # å»é‡é€»è¾‘ï¼šè·³è¿‡å·²ç»åŠ å…¥è¿‡çš„ foodName
        if final_name in seen:
            continue
        seen.add(final_name)

        output.append({
            "foodName": final_name,
            "quantity": qty,
            "unit": unit,
            "estimatedCategory": category,
            "estimatedWeight": est_weight
        })

    return {"parsedItems": output}




# ===================== 8) New: /api/image (Google Vision) =====================


# @app.post("/api/image")
# async def recognize_from_image(file: UploadFile):
#     """
#     1) Read uploaded image
#     2) Send to Google Cloud Vision API -> labelAnnotations
#     3) Convert top labels to "food1 and food2"
#     4) Reuse parse_text logic -> final foodItems
#     """
#     try:
#         # 1) read file content
#         contents = await file.read()
#         encoded = base64.b64encode(contents).decode('utf-8')

#         # 2) call Google Vision
#         # replace with your real API key
#         google_vision_api_key = "AIzaSyBRa0fP7e27Kz5-jHWegOA6EYZullXlvMg"
#         url = f"https://vision.googleapis.com/v1/images:annotate?key=AIzaSyBRa0fP7e27Kz5-jHWegOA6EYZullXlvMg"
#         request_body = {
#           "requests": [
#             {
#               "image": {"content": encoded},
#               "features": [{"type": "LABEL_DETECTION", "maxResults": 5}]
#             }
#           ]
#         }
#         resp = requests.post(url, json=request_body)
#         data = resp.json()
#         label_annotations = data.get("responses",[{}])[0].get("labelAnnotations", [])
#         # Just get top 2 or 3
#         top_labels = [ann["description"] for ann in label_annotations[:3]]

#         # 3) create a textual sentence, e.g. "apple and pizza"
#         # (User must manually set quantity => or default 1.0)
#         if top_labels:
#             sentence = " and ".join(top_labels)
#         else:
#             sentence = "unknown"

#         # 4) reuse parse_text logic
#         # but we just do: for each label => quantity=1.0 => let user fix
#         # or we can do a single string => "apple and pizza" => parse
#         # best approach is to parse => but that might fail if no numeric
#         # So let's just return them as if partial parse
#         # We'll do direct approach: no quantity
#         items = []
#         for lab in top_labels:
#             items.append({"foodName": lab.lower(), "quantity": 1.0})

#         # 5) do partial matching with NEVO
#         output = []
#         for it in items:
#             fName, qty = it["foodName"], it["quantity"]
#             matched = vector_match_food(fName)
#             if matched is None:
#                 output.append({
#                     "foodName": fName,
#                     "quantity": qty,
#                     "NEVO_matched": None,
#                     "error": "No semantic match found in NEVO"
#                 })
#                 continue
#             matched_row = nevo_df[nevo_df["CleanName"] == matched]
#             if matched_row.empty:
#                 output.append({
#                     "foodName": fName,
#                     "quantity": qty,
#                     "NEVO_matched": matched,
#                     "error": "Matched name not found in NEVO DB"
#                 })
#                 continue
#             row = matched_row.iloc[0]
#             info = get_nutrition_info(row, qty)
#             output.append({
#                 "foodName": fName,
#                 "quantity": qty,
#                 "NEVO_matched": row["Food name"],
#                 **info
#             })
#         sentence = " and ".join(top_labels) if top_labels else "unknown"
#         result = text_parse(TextInput(text=sentence))

#         # æ‰“å° Google Vision åŸå§‹æ ‡ç­¾ï¼Œæ–¹ä¾¿è°ƒè¯•
#         print("ğŸ“· top_labels:", top_labels)

#         sentence = " and ".join(top_labels) if top_labels else "unknown"
#         result   = text_parse(TextInput(text=sentence))

#         # æ‰“å°ç»è¿‡ text_parse åçš„è§£æç»“æœ
#         print("ğŸ“· parsedItems:", result)
#         return result 

#     except Exception as e:
#         import traceback
#         traceback.print_exc()
#         raise HTTPException(status_code=400, detail=str(e))
from fastapi import UploadFile, HTTPException
import base64
import requests
from pydantic import BaseModel

class TextInput(BaseModel):
    text: str

@app.post("/api/image")
async def recognize_from_image(file: UploadFile):
    try:
        # Step 1: è¯»å–ä¸Šä¼ å›¾åƒå¹¶ç¼–ç 
        contents = await file.read()
        encoded = base64.b64encode(contents).decode("utf-8")

        # Step 2: è°ƒç”¨ Google Cloud Vision APIï¼ˆæœ€å¤šè¿”å›5ä¸ªæ ‡ç­¾ï¼‰
        url = "https://vision.googleapis.com/v1/images:annotate?key=AIzaSyBRa0fP7e27Kz5-jHWegOA6EYZullXlvMg"
        request_body = {
            "requests": [{
                "image": {"content": encoded},
                "features": [{"type": "LABEL_DETECTION", "maxResults": 5}]
            }]
        }
        resp = requests.post(url, json=request_body)
        data = resp.json()

        # Step 3: æå–å‰5ä¸ªæ ‡ç­¾
        all_labels = [
            ann["description"]
            for ann in data.get("responses", [{}])[0].get("labelAnnotations", [])[:5]
        ]

        # Step 4: è¿‡æ»¤æ‰ä¸å…·ä½“çš„æ ‡ç­¾
        blacklist = {"cooking", "food", "produce", "processed", "cup", "tableware", "drinkware", "ingredient"}
        top_labels = [label for label in all_labels if label.lower() not in blacklist]

        sentence = " ".join(top_labels) if top_labels else "unknown"

        print("ğŸ“· åŸå§‹æ ‡ç­¾:", all_labels)
        print("ğŸ“· è¿‡æ»¤åæ ‡ç­¾:", top_labels)
        print("ğŸ“· å¤„ç†å¥å­:", sentence)

        # Step 5: ç”¨ parse_text ç²¾ç¡®åŒ¹é…
        parsed = parse_text(TextInput(text=sentence))
        parsed_items = parsed.get("parsedItems", [])

        # Step 6: æå–åŒ¹é…æˆåŠŸé¡¹
        matched_items = [item for item in parsed_items if item.get("estimatedCategory") is not None]

        if matched_items:
            print("âœ… ç²¾å‡†åŒ¹é…æˆåŠŸï¼Œè¿”å›åŒ¹é…é¡¹")
            return {"parsedItems": matched_items}
        else:
            print("âš ï¸ å…¨éƒ¨æœªåŒ¹é…ï¼Œä½¿ç”¨ text_parse fallback")
            return text_parse(TextInput(text=sentence))

    except Exception as e:
        import traceback
        traceback.print_exc()
        raise HTTPException(status_code=400, detail=str(e))


# ===================== 9) New: /api/audio (Whisper) =====================
# @app.post("/api/audio")
# async def recognize_from_audio(file: UploadFile = File(...)):
#     try:
#         input_path = tempfile.mktemp(suffix=".webm")
#         output_path = tempfile.mktemp(suffix=".wav")
#         with open(input_path, "wb") as f:
#             f.write(await file.read())

#         subprocess.run(["ffmpeg", "-i", input_path, output_path], check=True)

#         openai_api_key = "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"
#         transcribe_url = "https://api.openai.com/v1/audio/transcriptions"

#         with open(output_path, "rb") as audio_file:
#             files = {"file": ("converted.wav", audio_file, "audio/wav")}
#             headers = {"Authorization": f"Bearer {openai_api_key}"}
#             data = {
#             "model": "whisper-1",
#             "language": "en"
#            }

#             resp = requests.post(transcribe_url, headers=headers, files=files, data=data)

#         if resp.status_code != 200:
#             raise HTTPException(status_code=resp.status_code, detail=resp.text)

#         recognized_text = resp.json().get("text", "")
#         print("[ğŸ¤ Whisper è½¬æ–‡å­—ç»“æœ]:", recognized_text)

#         parse_input = TextInput(text=recognized_text)
#         result = parse_text(parse_input)
#         # return result
#         return text_parse(parse_input) 

#     except Exception as e:
#         print("[âŒ Whisper é”™è¯¯]:", e)
#         raise HTTPException(status_code=400, detail=str(e))



# @app.post("/api/audio")
# async def recognize_from_audio(file: UploadFile = File(...)):
#     try:
#         # å°†ä¸Šä¼ çš„ WebM ä¿å­˜ä¸ºä¸´æ—¶æ–‡ä»¶
#         input_path = tempfile.mktemp(suffix=".webm")
#         output_path = tempfile.mktemp(suffix=".wav")

#         with open(input_path, "wb") as f:
#             f.write(await file.read())

#         # ä½¿ç”¨ ffmpeg è½¬æ¢ä¸º WAV
#         subprocess.run(["ffmpeg", "-i", input_path, output_path], check=True)

#         # è°ƒç”¨ Whisper æ¥å£è¯†åˆ«æ–‡å­—
#         openai_api_key = "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"
#         with open(output_path, "rb") as audio_file:
#             resp = requests.post(
#                 "https://api.openai.com/v1/audio/transcriptions",
#                 headers={"Authorization": f"Bearer {openai_api_key}"},
#                 files={"file": ("audio.wav", audio_file, "audio/wav")},
#                 data={"model": "whisper-1", "language": "en"}
#             )

#         if resp.status_code != 200:
#             raise HTTPException(status_code=resp.status_code, detail=resp.text)

#         recognized_text = resp.json().get("text", "")

#         print("[ğŸ¤ Whisperç»“æœ]", recognized_text)

#         # ç›´æ¥è°ƒç”¨è½»é‡è§£æé€»è¾‘ï¼ˆè¿”å› parsedItemsï¼‰
#         from pydantic import BaseModel
#         class TextInput(BaseModel):
#             text: str
#         return text_parse(TextInput(text=recognized_text))

#     except Exception as e:
#         print("[âŒ Whisperé”™è¯¯]:", e)
#         raise HTTPException(status_code=400, detail=str(e))

# ===================== 9) New: /api/audio (Whisper ç›´æ¥å¤„ç† .webm) =====================
# from fastapi import UploadFile, File, HTTPException
# import requests, traceback

# @app.post("/api/audio")
# async def recognize_from_audio(file: UploadFile = File(...)):
#     """
#     1) è¯»å–å‰ç«¯ä¸Šä¼ çš„ .webm éŸ³é¢‘
#     2) ç›´æ¥ä¸Šä¼ ç»™ OpenAI Whisperï¼ˆWhisper å®˜æ–¹æ”¯æŒ webmï¼‰
#     3) å°†è¯†åˆ«æ–‡æœ¬é€å…¥ text_parseï¼Œä¿æŒè¿”å›ç»“æ„ä¸å˜
#     """
#     try:
#         # 1ï¸âƒ£ è¯»å– bytes
#         audio_bytes = await file.read()

#         # 2ï¸âƒ£ Whisper è¯­éŸ³è½¬æ–‡å­—
#         openai_api_key = (
#             "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"
#         )

#         resp = requests.post(
#             "https://api.openai.com/v1/audio/transcriptions",
#             headers={"Authorization": f"Bearer {openai_api_key}"},
#             files={
#                 # filename ä»»æ„ï¼ŒMIME å¿…é¡» audio/webm
#                 "file": ("audio.webm", audio_bytes, "audio/webm")
#             },
#             data={"model": "whisper-1", "language": "en"}
#         )

#         if resp.status_code != 200:
#             raise HTTPException(status_code=resp.status_code, detail=resp.text)

#         recognized_text = resp.json().get("text", "")
#         print("[ğŸ¤ Whisperç»“æœ]", recognized_text)

#         # 3ï¸âƒ£ è½»é‡è§£æ â†’ è¿”å› parsedItemsï¼Œä¸åŸé€»è¾‘ä¸€è‡´
#         from pydantic import BaseModel
#         class TextInput(BaseModel):
#             text: str

#         return text_parse(TextInput(text=recognized_text))

#     except Exception as e:
#         print("[âŒ Whisperé”™è¯¯]:", e)
#         traceback.print_exc()
#         raise HTTPException(status_code=400, detail=str(e))

# from fastapi import UploadFile, File, HTTPException
# import requests
# import traceback
# from pydantic import BaseModel

# class TextInput(BaseModel):
#     text: str

# @app.post("/api/audio")
# async def recognize_from_audio(file: UploadFile = File(...)):
#     """
#     å¤šè®¾å¤‡éŸ³é¢‘ä¸Šä¼ å…¼å®¹æ¥å£ï¼ˆæ—  ffmpegï¼‰ï¼š
#     - æ”¯æŒ .webm, .mp4, .m4aï¼ˆiPhoneï¼‰, .aac
#     - è‡ªåŠ¨åˆ¤æ–­ MIME ç±»å‹
#     - å‘é€è‡³ OpenAI Whisper è¿›è¡Œè¯†åˆ«
#     - è¿”å› text_parse çš„ç»“æ„
#     """
#     try:
#         # 1ï¸âƒ£ è¯»å–éŸ³é¢‘å†…å®¹
#         audio_bytes = await file.read()
#         print("ğŸ“¦ æ–‡ä»¶å¤§å°:", len(audio_bytes), "bytes")

#         if len(audio_bytes) < 500:
#             raise HTTPException(status_code=400, detail="éŸ³é¢‘å¤ªçŸ­æˆ–æ— æ•ˆ")
#         if len(audio_bytes) > 2 * 1024 * 1024:
#             raise HTTPException(status_code=413, detail="éŸ³é¢‘æ–‡ä»¶è¿‡å¤§ï¼ˆå»ºè®®å°äº2MBï¼‰")

#         # 2ï¸âƒ£ æ£€æµ‹çœŸå® MIME ç±»å‹
#         real_mime = file.content_type
#         print("ğŸ“¦ å®é™… MIME ç±»å‹:", real_mime)

#         # 3ï¸âƒ£ è®¾ç½®ä¸Šä¼ ç»™ Whisper çš„ MIME å’Œæ–‡ä»¶å
#         if real_mime in ["audio/mp4", "video/mp4", "audio/aac", "audio/x-m4a", "audio/m4a"]:
#             filename = "audio.m4a"
#             whisper_mime = "audio/m4a"
#         else:
#             filename = "audio.webm"
#             whisper_mime = "audio/webm"

#         # 4ï¸âƒ£ Whisper API è¯·æ±‚
#         openai_api_key = "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"  # â—è¯·æ›¿æ¢ä¸ºä½ çš„æœ‰æ•ˆ OpenAI API key
#         resp = requests.post(
#             "https://api.openai.com/v1/audio/transcriptions",
#             headers={"Authorization": f"Bearer {openai_api_key}"},
#             files={"file": (filename, audio_bytes, whisper_mime)},
#             data={"model": "whisper-1", "language": "en"},
#             timeout=15
#         )

#         print("ğŸ“¡ Whisperå“åº”ç :", resp.status_code)
#         print("ğŸ“¡ Whisperå“åº”ä½“:", resp.text)

#         if resp.status_code != 200:
#             raise HTTPException(status_code=resp.status_code, detail=resp.text)

#         # 5ï¸âƒ£ æå– Whisper è¿”å›çš„æ–‡æœ¬
#         recognized_text = resp.json().get("text", "")
#         print("[ğŸ¤ Whisperè¯†åˆ«ç»“æœ]:", recognized_text)

#         # 6ï¸âƒ£ è°ƒç”¨ text_parse è¿”å›ç»“æ„
#         return text_parse(TextInput(text=recognized_text))

#     except Exception as e:
#         print("[âŒ Whisperå¤„ç†å¼‚å¸¸]:", e)
#         traceback.print_exc()
#         raise HTTPException(status_code=400, detail=str(e))

# from fastapi import UploadFile, File, HTTPException
# from pydantic import BaseModel
# import requests
# import traceback

# import magic

# SUPPORTED_MIME_MAP = {
#     "audio/webm": "webm",
#     "audio/mpeg": "mp3",
#     "audio/mp3": "mp3",
#     "audio/mp4": "mp4",
#     "audio/x-m4a": "m4a",
#     "audio/m4a": "m4a",
#     "video/mp4": "mp4",
#     "audio/aac": "m4a",
#     "audio/wav": "wav",
#     "audio/ogg": "ogg",
#     "audio/oga": "ogg",
#     "audio/mpga": "mp3",
#     "audio/flac": "flac"
# }

# @app.post("/api/audio")
# async def recognize_from_audio(file: UploadFile = File(...)):
#     try:
#         audio_bytes = await file.read()
#         print("ğŸ“¦ æ–‡ä»¶å¤§å°:", len(audio_bytes), "bytes")

#         if len(audio_bytes) < 500:
#             raise HTTPException(status_code=400, detail="éŸ³é¢‘å¤ªçŸ­æˆ–æ— æ•ˆ")
#         if len(audio_bytes) > 2 * 1024 * 1024:
#             raise HTTPException(status_code=413, detail="éŸ³é¢‘æ–‡ä»¶è¿‡å¤§ï¼ˆå»ºè®®å°äº2MBï¼‰")

#         # 1ï¸âƒ£ magic æ£€æµ‹çœŸå® MIME ç±»å‹
#         mime = magic.Magic(mime=True)
#         real_mime = mime.from_buffer(audio_bytes[:2048])
#         print("ğŸ“¦ magic æ£€æµ‹ MIME ç±»å‹:", real_mime)

#         if real_mime in SUPPORTED_MIME_MAP:
#             file_ext = SUPPORTED_MIME_MAP[real_mime]
#             filename = f"audio.{file_ext}"
#             whisper_mime = f"audio/{file_ext}"
#         else:
#             # å¼ºåˆ¶ fallback ä¸º webm
#             print(f"âš ï¸ æœªçŸ¥ MIME ç±»å‹ {real_mime}ï¼Œå¼ºåˆ¶è®¾ä¸º webm")
#             filename = "audio.webm"
#             whisper_mime = "audio/webm"

#         # 2ï¸âƒ£ Whisper è¯·æ±‚
#         openai_api_key = "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"  # æ›¿æ¢ä¸ºä½ çš„ key
#         resp = requests.post(
#             "https://api.openai.com/v1/audio/transcriptions",
#             headers={"Authorization": f"Bearer {openai_api_key}"},
#             files={"file": (filename, audio_bytes, whisper_mime)},
#             data={"model": "whisper-1", "language": "en"},
#             timeout=15
#         )

#         print("ğŸ“¡ Whisperå“åº”ç :", resp.status_code)
#         print("ğŸ“¡ Whisperå“åº”ä½“:", resp.text)

#         if resp.status_code != 200:
#             raise HTTPException(status_code=resp.status_code, detail=resp.text)

#         recognized_text = resp.json().get("text", "")
#         print("[ğŸ¤ Whisperè¯†åˆ«ç»“æœ]:", recognized_text)

#         return text_parse(TextInput(text=recognized_text))

#     except Exception as e:
#         print("[âŒ Whisperå¤„ç†å¼‚å¸¸]:", e)
#         traceback.print_exc()
#         raise HTTPException(status_code=400, detail=str(e))

from fastapi import UploadFile, File, HTTPException
from pydantic import BaseModel
import tempfile
import subprocess
import requests
import traceback
import magic   # python-magic
# ä½ çš„å…¶ä½™ importï¼ˆnevo_dfã€vector_match_foodã€text_parseã€TextInput ç­‰ï¼‰ä¿æŒä¸å˜

SUPPORTED_MIME_MAP = {
    "audio/webm": "webm",
    "audio/mpeg": "mp3",
    "audio/mp3": "mp3",
    "audio/mp4": "mp4",
    "audio/x-m4a": "m4a",
    "audio/m4a": "m4a",
    "video/mp4": "mp4",            # è¯†åˆ«å¹¶å•ç‹¬å¤„ç†
    "audio/aac": "m4a",
    "audio/wav": "wav",
    "audio/ogg": "ogg",
    "audio/oga": "ogg",
    "audio/mpga": "mp3",
    "audio/flac": "flac"
}

@app.post("/api/audio")
async def recognize_from_audio(file: UploadFile = File(...)):
    try:
        audio_bytes = await file.read()
        print("ğŸ“¦ æ–‡ä»¶å¤§å°:", len(audio_bytes), "bytes")

        if len(audio_bytes) < 500:
            raise HTTPException(status_code=400, detail="éŸ³é¢‘å¤ªçŸ­æˆ–æ— æ•ˆ")
        if len(audio_bytes) > 2 * 1024 * 1024:
            raise HTTPException(status_code=413, detail="éŸ³é¢‘æ–‡ä»¶è¿‡å¤§ï¼ˆå»ºè®®å°äº2MBï¼‰")

        # 1ï¸âƒ£ magic æ£€æµ‹çœŸå® MIME ç±»å‹
        mime = magic.Magic(mime=True)
        real_mime = mime.from_buffer(audio_bytes[:2048])
        print("ğŸ“¦ magic æ£€æµ‹ MIME ç±»å‹:", real_mime)

        # 2ï¸âƒ£ video/mp4 -> è½¬ WAVï¼›å…¶ä½™ä¿æŒåŸæ¥é€»è¾‘
        if real_mime == "video/mp4":
            # ä¿å­˜ä¸´æ—¶ mp4
            tmp_mp4 = tempfile.NamedTemporaryFile(delete=False, suffix=".mp4")
            tmp_mp4.write(audio_bytes)
            tmp_mp4.close()

            # è½¬ç ä¸ºå•å£°é“ 16 kHz WAV
            tmp_wav = tempfile.NamedTemporaryFile(delete=False, suffix=".wav")
            tmp_wav.close()

            print("ğŸ”„ ffmpeg è½¬ç  video/mp4 â†’ wav")
            subprocess.run(
                ["ffmpeg", "-y", "-i", tmp_mp4.name, "-vn",
                 "-ar", "16000", "-ac", "1", tmp_wav.name],
                check=True,
                stdout=subprocess.DEVNULL,
                stderr=subprocess.DEVNULL
            )
            with open(tmp_wav.name, "rb") as f:
                audio_bytes = f.read()
            filename = "converted.wav"
            whisper_mime = "audio/wav"
        else:
            # å…¶ä½™æ ¼å¼æ²¿ç”¨åŸæ¥é€»è¾‘
            if real_mime in SUPPORTED_MIME_MAP:
                ext = SUPPORTED_MIME_MAP[real_mime]
                filename = f"audio.{ext}"
                whisper_mime = f"audio/{ext}"
            else:
                print(f"âš ï¸ æœªçŸ¥ MIME ç±»å‹ {real_mime}ï¼Œå¼ºåˆ¶è®¾ä¸º webm")
                filename = "audio.webm"
                whisper_mime = "audio/webm"

        # 3ï¸âƒ£ è°ƒç”¨ OpenAI Whisper
        openai_api_key = "sk-ä½ çš„KEY"
        resp = requests.post(
            "https://api.openai.com/v1/audio/transcriptions",
            headers={"Authorization": f"Bearer {openai_api_key}"},
            files={"file": (filename, audio_bytes, whisper_mime)},
            data={"model": "whisper-1", "language": "en"},
            timeout=15
        )

        print("ğŸ“¡ Whisperå“åº”ç :", resp.status_code)
        print("ğŸ“¡ Whisperå“åº”ä½“:", resp.text)

        if resp.status_code != 200:
            raise HTTPException(status_code=resp.status_code, detail=resp.text)

        recognized_text = resp.json().get("text", "")
        print("[ğŸ¤ Whisperè¯†åˆ«ç»“æœ]:", recognized_text)

        return text_parse(TextInput(text=recognized_text))

    except Exception as e:
        print("[âŒ Whisperå¤„ç†å¼‚å¸¸]:", e)
        traceback.print_exc()
        raise HTTPException(status_code=400, detail=str(e))

# ===================== 10) New: /api/confirm (User-edited Items) =====================
class FoodItem(BaseModel):
    foodName: str
    quantity: float

@app.post("/api/confirm")
def confirm_food_items(items: List[FoodItem]):
    """
    1) front-end sends final list of {foodName, quantity}
    2) we do NEVO fuzzy match + nutrition => return
    3) if mismatch => NEVO_matched=null
    """
    output = []
    for it in items:
        fName = it.foodName.strip().lower()
        qty = it.quantity
        matched = vector_match_food(fName)
        if matched is None:
            output.append({
                "foodName": fName,
                "quantity": qty,
                "NEVO_matched": None,
                "error": "No semantic match in NEVO"
            })
            continue

        matched_row = nevo_df[nevo_df["CleanName"] == matched]
        if matched_row.empty:
            output.append({
                "foodName": fName,
                "quantity": qty,
                "NEVO_matched": matched,
                "error": "Row not found in NEVO"
            })
            continue

        row = matched_row.iloc[0]
        info = get_nutrition_info(row, qty)
        output.append({
            "foodName": fName,
            "quantity": qty,
            "NEVO_matched": row["Food name"],
            **info
        })

    # return {"foodItems": output}
    return { "parsedItems": output }


# ===================== Original root endpoint =====================
@app.get("/")
def root():
    return {"message": "Extended NEVO backend running with text/image/audio & confirm."}


# ===================== MAIN LAUNCH =====================
if __name__ == "__main__":
    import uvicorn
    uvicorn.run("main:app", reload=True)
